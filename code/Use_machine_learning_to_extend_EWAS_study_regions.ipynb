{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from common import commons\n",
    "from common import Convert_To_Normal_Dist as ctnd\n",
    "from common import DataScaler as ds\n",
    "home = commons.home\n",
    "from log import Logger\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,cross_validate\n",
    "from features_selection import TTest\n",
    "from matplotlib import pyplot as plt\n",
    "from features_selection import CorrFeatureSelector as cfs\n",
    "from simulation import AutoencoderSimulator as AS\n",
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from features_selection import Feature_Selection as FS\n",
    "from feature_engineering import Sparse_Autoencoder_class as sac\n",
    "from hyperopt import fmin,tpe,hp, STATUS_OK,Trials\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "sys.path.append('/home/ec2-user/anaconda3/lib/python3.6/site-packages')\n",
    "from models import ModelSelection as MS\n",
    "from models import Ensemble as es\n",
    "from models import xgbooster\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_score,accuracy_score,f1_score,roc_curve,roc_auc_score,precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import clone\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from importlib import reload\n",
    "from models import deep_network_estimator as dne\n",
    "from models import Ensemble_hyperopt as eh\n",
    "from hyperopt import fmin,tpe,hp, STATUS_OK,Trials\n",
    "from hyperopt_models import parallel_ensemble as pe\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import prediction_commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sites selection\n",
    "##collect positive training set and negative training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_dir = home_path+'logs/'\n",
    "logger = Logger.Logger(log_dir,True).get_logger()\n",
    "pos_pvalue = 0.0001\n",
    "neg_pvalue = 0.1\n",
    "sample_ratio_neg_to_pos = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger.info('Positive pvalue: %f, \\t Control pvalue: %f,\\t Positive/Control ratio: %f',pos_pvalue,neg_pvalue,sample_ratio_neg_to_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import 'allsites.xlsx'\n",
    "all_sites = pd.read_excel(home +'data/RICHS/allsites.xlsx','Excel Table S4',skiprows=4,header=None, names=['id','chr','coordinate','beta_sign','pvalue'],usecols=[0,1,2,5,6])\n",
    "logger.info('Datasets location: '+ home +'data/RICHS/allsites.xlsx')\n",
    "betas = pd.read_csv(home +'data/RICHS/RICHS_betaValue_summary.csv',skiprows=1,header=None,usecols=[0,1],names=['id','beta_mean'])\n",
    "all_sites.sort_values(['id'],inplace=True)\n",
    "betas.sort_values(['id'],inplace=True)\n",
    "all_sites = pd.merge(all_sites,betas,on=['id'],how='left')\n",
    "all_sites.rename(columns={'beta_mean':'beta'},inplace=True)\n",
    "all_sites.sort_values(['pvalue'],inplace=True,ascending=True)\n",
    "#split positive and negative sites\n",
    "positive_sites = all_sites.query('pvalue<=@pos_pvalue')\n",
    "positive_sites['label'] = np.where(positive_sites['beta_sign']>0,1,-1)\n",
    "negative_sites = all_sites.query('pvalue>@neg_pvalue')\n",
    "negative_sites['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select negative sites\n",
    "select_negs_list = []\n",
    "negatives_sort_by_beta = negative_sites.sort_values(['beta'])\n",
    "hyper_sites = negatives_sort_by_beta.query('beta_sign>=0')\n",
    "hypo_sites = negatives_sort_by_beta.query('beta_sign<0')\n",
    "for beta,beta_sign in positive_sites[['beta','beta_sign']].values:\n",
    "    tmp_sites = hyper_sites if beta_sign >=0 else hypo_sites\n",
    "    neg_ix = tmp_sites['beta'].searchsorted(beta)[0]    \n",
    "    negs = tmp_sites.iloc[neg_ix-int(sample_ratio_neg_to_pos/2):np.minimum(neg_ix+int(sample_ratio_neg_to_pos/2),len(negatives_sort_by_beta)),:]\n",
    "    select_negs_list.extend(negs.values)\n",
    "select_negs = pd.DataFrame(select_negs_list,columns=['id','chr','coordinate','beta_sign','pvalue','beta','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add 'win id'\n",
    "win_path = home_path+'wins.txt'\n",
    "pos_sites_with_winid, neg_sites_with_winid = commons.merge_with_feature_windows(win_path,positive_sites,select_negs)\n",
    "all_sites_with_winid = pos_sites_with_winid.append(neg_sites_with_winid,ignore_index=True)\n",
    "all_sites_with_winid.drop_duplicates(['id'],inplace=True)\n",
    "all_sites_with_winid.sort_values(['chr','coordinate'],inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save as 'all_sites_winind.csv'\n",
    "with pd.HDFStore(home +'data/RICHS/all_sites_winid','w') as h5s:\n",
    "    h5s['all_sites_winid'] = all_sites_with_winid\n",
    "       all_sites_with_winid.to_csv(home_path+'data/RICHS/all_sites_winid.csv',index=False)  \n",
    "all_sites_with_winid['winid'].to_csv(home +'data/RICHS/selected_pos_winid.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions defination\n",
    "#--------------------------------------------------------------------- \n",
    "def normalize_feature(all_features,dataset='AD_CpG'):\n",
    "    count_thresh = 10\n",
    "    unique_value_counts = [len(all_features[col].unique()) for col in all_features.columns[2:]] #ignore chr and coordinate\n",
    "    normalize_all_features = all_features.copy()\n",
    "    mappings = pd.DataFrame()\n",
    "    for col,num in zip(all_features.columns[2:],unique_value_counts):\n",
    "        if num >= count_thresh:\n",
    "            print(col)\n",
    "            atn = ctnd.AnyToNormal(col)\n",
    "            mapping = atn.transform(normalize_all_features)\n",
    "            mappings[mapping.columns[0]] = mapping.ix[:,0]\n",
    "            mappings[mapping.columns[1]] = mapping.ix[:,1]\n",
    "        else: \n",
    "            continue\n",
    "    h5s = pd.HDFStore(home+'data/'+dataset+'/normalize_mapping','w')\n",
    "    h5s['mappings'] = mappings\n",
    "    h5s['normal_features'] = normalize_all_features\n",
    "    h5s.close()\n",
    "    return home+'data/'+dataset+'/normalize_mapping',normalize_all_features,mappings\n",
    "   \n",
    "#-------------------------------------------------------------------------------\n",
    "def data_selection(data,classes=[0,1,-1],combine=False,*args):              #which classes of data to keep\n",
    "    if not isinstance(data,pd.DataFrame):\n",
    "        cols = args\n",
    "        data = pd.DataFrame(data,columns=cols)    \n",
    "    ret_data = data.query('label in @classes')\n",
    "    for i,label in enumerate(classes):\n",
    "        ret_data['label'][ret_data['label']==label] = i\n",
    "    \n",
    "    if len(classes)>2 and combine==True:\n",
    "        ret_data['label'] = ret_data['label'].where(ret_data['label']==0,1)\n",
    "    return ret_data\n",
    "        \n",
    "#------------------------------------------------------------------------------                  \n",
    " \n",
    "def TSNEPlot(data,class_labels,param_map):##param_map like 1:('r','o','80')\n",
    "    data1 = data.reset_index()\n",
    "    tsne = TSNE(n_components=3,random_state=91)\n",
    "    feature_reduced = tsne.fit_transform(data1.drop(['label','index'],axis=1))\n",
    "    fig = plt.figure(figsize=(18,15))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    for i in class_labels:\n",
    "        c,marker,size = param_map[i]\n",
    "        print(c,marker,size)\n",
    "        index = data1.query('label == @i').index.values\n",
    "        ax.scatter(feature_reduced[index,0],feature_reduced[index,1],feature_reduced[index,2],c=c,marker=marker,s=size)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def simulate(data,label,**argkw):\n",
    "    data = data[data['label']==label]\n",
    "    encoder = AS.dataset_simulator(**argkw)\n",
    "    encoder.fit(np.array(data.drop('label',axis=1)))\n",
    "    sim_data = pd.DataFrame(encoder.transform(),columns=data.columns.drop('label'))\n",
    "    sim_data['label'] = label\n",
    "    return sim_data\n",
    "          \n",
    "#-----------------------------------------------------------------------------\n",
    "def subset_control(data,num):      ## randomly select a subset of certain ratio positive vs control samples \n",
    "    pos_index = data.query('label!=0').index.values\n",
    "    control_sub_index = np.random.permutation(data.query('label==0').index.values)[:len(pos_index)*num]\n",
    "    select_index = np.concatenate((pos_index,control_sub_index))\n",
    "    return data.loc[select_index,:]\n",
    "#------------------------------------------------------------------------------\n",
    "def sparse_autoencoder_score(ae_params):\n",
    "    global encoders\n",
    "    sparse_ae = sac.sparse_autoencoder(**ae_params)\n",
    "    score = commons.cross_validate_score(sparse_ae,reduced_train_x,train_label,ae_sample_weights_train)\n",
    "    encoders.extend([sparse_ae])\n",
    "    if math.isnan(score) or math.isfinite(score):\n",
    "        score = np.Infinity\n",
    "    return {'loss':-score,'status':STATUS_OK}\n",
    "#-----------------------------------------------------------------\n",
    "def selected_feature_analysis(features,X,y):\n",
    "    t_stats = []\n",
    "    pos = X[y==1]\n",
    "    neg = X[y==0]\n",
    "    for feature in features:\n",
    "        test = TTest.FeatureTTest(feature)\n",
    "        t_stats.extend([test.fit(pos,neg)])\n",
    "    return pd.DataFrame(t_stats)\n",
    "#--------------------------------------------------------------------\n",
    "def method_params(methods=['random_forest','xgboost','logistic_regression','linear_SVC']):\n",
    "    params={}\n",
    "    feature_num = train_x.shape[1]\n",
    "    #class_weight = {0:1,1:30}\n",
    "    class_weight = None\n",
    "    l_param={'C':9,'penalty':'l1'}\n",
    "    rf_param = {'n_estimators':1000,'max_depth':10,'min_samples_split':14 ,'min_samples_leaf':1, 'n_jobs':-1 }\n",
    "    svc_param = {'C':2,'dual':False,'penalty':'l1'}\n",
    "    xgb_param = {'learning_rate':0.1,'max_depth':10,'n_estimators':1500,'reg_lambda':40,'gamma':1,'n_jobs':-1}\n",
    "    mutual_information_param = {'k':100}\n",
    "    fisher_param = {'k':100}\n",
    "    if 'logistic_regression' in methods:\n",
    "        params['logistic_regression'] = l_param\n",
    "    if 'random_forest' in methods:\n",
    "        params['random_forest'] = rf_param\n",
    "    if 'linear_SVC' in methods:\n",
    "        params['linear_SVC'] = svc_param\n",
    "    if 'xgboost' in methods:\n",
    "        params['xgboost'] = xgb_param\n",
    "    if 'mutual_information' in methods:\n",
    "        params['mutual_information'] = mutual_information_param\n",
    "    if 'fisher_score' in methods:\n",
    "        params['fisher_score'] = fisher_param   \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import 'all_features' data\n",
    "dataset = 'RICHS'\n",
    "log_dir = home+'logs/'\n",
    "logger = Logger.Logger(log_dir,False).get_logger()\n",
    "with pd.HDFStore(home+'/data/'+dataset+'/all_features','r') as h5s:\n",
    "    all_data = h5s['all_features']\n",
    "all_data['beta_sign'] = all_data['label']\n",
    "all_data.drop(['coordinate','chr'],axis=1,inplace=True)\n",
    "all_data['dist_to_nearest_tss'] = all_data['dist_to_nearest_tss'].astype('i8')\n",
    "all_data = data_selection(all_data,classes=[0,1,-1],combine=True)\n",
    "all_features = all_data\n",
    "#all_features = subset_control(all_data,30)\n",
    "#all_features = all_data.query('beta_sign>0') ##only for hypermethylated sites in RICHS dataset, for AD dataset, hyper/hypo status can't be determined from beta\n",
    "#logger.info('only keep heypermethylated sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into train and test data and scaling on train data\n",
    "scaler='MinMax'\n",
    "all_features.drop(['id','winid','beta','beta_sign'],axis=1,inplace=True)\n",
    "train_x,train_label,test_x,test_label = commons.train_test_split(all_features,scaler=scaler)\n",
    "train_x.reset_index(drop=True,inplace=True)\n",
    "train_label.reset_index(drop=True,inplace=True)\n",
    "test_x.reset_index(drop=True,inplace=True)\n",
    "test_label.reset_index(drop=True,inplace=True)\n",
    "logger.info('Data scaler type is: %s',scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get sample weights\n",
    "sample_weights_train = commons.sample_weights(train_x,train_label,factor=1)\n",
    "sample_weights_test = commons.sample_weights(test_x,test_label,factor=1)\n",
    "weight_min_max_ratio = sample_weights_train.max()/sample_weights_train.min()\n",
    "train_x.drop(['pvalue'],axis=1,inplace=True)\n",
    "test_x.drop(['pvalue'],axis=1,inplace=True)\n",
    "logger.info('weight max ratio: %f',weight_min_max_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use 'FeatureSelection' function do feature selection\n",
    "fs_sample_weights = np.power(sample_weights_train,0.5) if dataset == 'RICHS' else sample_weights_train\n",
    "methods = ['random_forest','xgboost','logistic_regression','linear_SVC']\n",
    "all_intersect = False\n",
    "fs_params = method_params(methods)\n",
    "fs = FS.FeatureSelection(class_num=2,methods=methods,all_intersect=all_intersect,**fs_params)\n",
    "logger.info('Feature selection methods are: '+str(methods))\n",
    "logger.info('All intersected features: '+str(all_intersect))\n",
    "fs.fit(sample_weight=fs_sample_weights)\n",
    "selected_features = fs.transform(train_x,train_label)\n",
    "logger.info('selected features number is: %d\\n',selected_features.shape[0])\n",
    "logger.info(selected_features)\n",
    "reduced_train_x = train_x[selected_features['feature']]\n",
    "reduced_test_x = test_x[selected_features['feature']]\n",
    "total_x = pd.concat([reduced_train_x,reduced_test_x],ignore_index=True)\n",
    "total_label = pd.concat([train_label,test_label],ignore_index=True)\n",
    "total_weights = pd.concat([sample_weights_train,sample_weights_test],ignore_index=True)\n",
    "#only choose 60 features from the selected features\n",
    "feature_diff_stats = selected_feature_analysis(selected_features['feature'],total_x,total_label)\n",
    "feature_diff_stats = pd.merge(feature_diff_stats,selected_features,on='feature')\n",
    "selected_features_100 = feature_diff_stats if len(feature_diff_stats) <=50 else feature_diff_stats.sort_values(['n','pvalue'],ascending=[False,True])[:60]\n",
    "reduced_train_x = train_x[selected_features_100['feature']]\n",
    "reduced_test_x = test_x[selected_features_100['feature']]\n",
    "total_x = pd.concat([reduced_train_x,reduced_test_x],ignore_index=True)\n",
    "total_label = pd.concat([train_label,test_label],ignore_index=True)\n",
    "total_weights = pd.concat([sample_weights_train,sample_weights_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features_100.to_csv(home+'data/'+dataset+'/feature_stas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the selected 60 features as 'selected_features'\n",
    "with pd.HDFStore(home+'data/'+dataset+'/selected_features','w') as h5s:\n",
    "    h5s['train_x'] = reduced_train_x\n",
    "    h5s['train_label'] = train_label\n",
    "    h5s['test_x'] = reduced_test_x\n",
    "    h5s['test_label'] = test_label\n",
    "    h5s['sample_weights_train'] = sample_weights_train\n",
    "    h5s['sample_weights_test'] = sample_weights_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions defination\n",
    "#------------------------------------------------------------------------------------\n",
    "def plot_curves_cv(probs,label,methods,types='roc_curve'):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(types)\n",
    "    plt.axis([0,1,0,1])\n",
    "    lw = 2\n",
    "    colors = ['r','b','g','k','c','m','y']\n",
    "    for method,color in zip(methods,colors[:len(methods)]):\n",
    "        if types == 'precision_recall_curve':       \n",
    "            precision,recall,threshold = precision_recall_curve(label,probs[method])\n",
    "            plt.plot(recall,precision,color,linewidth=2,label=method)\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "        if types == 'roc_curve':\n",
    "            fpr,tpr, threshold = roc_curve(label,probs[method])\n",
    "            plt.plot(fpr,tpr,color,linewidth=2,label=method)\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('Recall')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "def get_estimators(methods,params,train_x,train_label):\n",
    "    ensemble = eh.Ensemble(methods,params)\n",
    "    ensemble.fit(train_x,train_label,sample_weight=sample_weight_train,max_iter=100)\n",
    "    return ensemble\n",
    "#-----------------------------------------------------------------------------\n",
    "def plot_curves(estimators,test_x,label,types='roc_curve'):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(types)\n",
    "    plt.axis([0,1,0,1])\n",
    "    lw = 2\n",
    "    colors = ['r','b','g','k','c','m','y']\n",
    "    for color,estimator in zip(colors[:len(estimators)],estimators):\n",
    "        name = type(estimator).__name__\n",
    "        probs = np.array(estimator.predict_proba(test_x))[:,1]\n",
    "        if types == 'precision_recall_curve':       \n",
    "            precision,recall,threshold = precision_recall_curve(label,probs)\n",
    "            plt.plot(recall,precision,color,linewidth=2,label=name)\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "        if types == 'roc_curve':\n",
    "            fpr,tpr, threshold = roc_curve(label,probs)\n",
    "            plt.plot(fpr,tpr,color,linewidth=2,label=name)\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('Recall')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.legend(loc='best')\n",
    "#-----------------------------------------------------------------------------\n",
    "def learn_curve(model,train_x,train_label,cv=3,scoring='neg_log_loss'):\n",
    "    model_c = clone(model)\n",
    "    N,train_score,test_score = learning_curve(model_c, \n",
    "                                            train_x,train_label,cv=cv,train_sizes=np.linspace(0.5,1,5),\n",
    "                                            scoring=scoring)\n",
    "    \n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.title('{}'.format(type(model).__name__))\n",
    "    plt.plot(N,np.mean(train_score,1),color='blue', label='training score')\n",
    "    plt.plot(N,np.mean(test_score,1),color='red',label='validation score')\n",
    "    plt.xlabel('training sample')\n",
    "    plt.ylabel(scoring)\n",
    "    plt.legend(loc=0)\n",
    "    plt.show()\n",
    "#-----------------------------------------------------------------------------    \n",
    "def error_analysis(estimator,test_x,label,types=['confusion_matrix']):\n",
    "    print('Error analysis of ',type(estimator).__name__)\n",
    "    predict = estimator.predict(test_x)\n",
    "    probs = np.array(estimator.predict_proba(test_x))[:,1]\n",
    "    class_num = len(label.unique())\n",
    "    if 'confusion_matrix' in types:\n",
    "        conf_mat = confusion_matrix(label,predict)\n",
    "        row_sums = conf_mat.sum(axis=1,keepdims=True)\n",
    "        norm_conf_mat = conf_mat/row_sums\n",
    "        np.fill_diagonal(norm_conf_mat,0)\n",
    "        plt.matshow(norm_conf_mat,cmap=plt.cm.gray)        \n",
    "    if 'precision_recall_curve' in types and class_num<=2:\n",
    "        precision,recall,threshold = precision_recall_curve(label,probs)\n",
    "        plot_curve(recall,precision,type(estimator).__name__,'precision_recall_curve')\n",
    "    if 'roc_curve' in types and class_num<=2:\n",
    "        fpr,tpr, threshold = roc_curve(label,probs)\n",
    "        plot_curve(fpr,tpr,type(estimator).__name__,'roc_curve')    \n",
    "    plt.show()\n",
    "#---------------------------------------------------------------------------    \n",
    "def plot_curve(score1,score2,label,types):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(types)\n",
    "    plt.plot(score1,score2,linewidth=2,label=label)\n",
    "    plt.plot([0,1],[1,1],'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    lw = 2\n",
    "    if types == 'precision_recall_curve':        \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "    if types == 'roc_curve':\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "#--------------------------------------------------------------------------\n",
    "def scores(y,predicts,pred_probs,average='macro'):\n",
    "    class_num = len(y.unique())\n",
    "    score_map = {}\n",
    "\n",
    "    if class_num <=2:\n",
    "        recall = recall_score(y,predicts)      \n",
    "        precision = precision_score(y,predicts)      \n",
    "        accuracy = accuracy_score(y,predicts)\n",
    "        f1 = f1_score(y,predicts)\n",
    "        auc = roc_auc_score(y,pred_probs)\n",
    "        score_map['auc'] = auc\n",
    "    else:\n",
    "        recall = recall_score(y,predicts,average=average)      \n",
    "        precision = precision_score(y,predicts,average=average)      \n",
    "        accuracy = accuracy_score(y,predicts)\n",
    "        f1 = f1_score(y,predicts,average=average)\n",
    "        \n",
    "    score_map['recall'] = recall\n",
    "    score_map['precision'] = precision\n",
    "    score_map['accuracy'] = accuracy\n",
    "    score_map['f1'] = f1\n",
    "    return score_map\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "def cross_val_ensemble(x,y,sample_weight,methods,params,fold=10, hyperopt=True,up_sampling=False):\n",
    "    skfolds = StratifiedKFold(n_splits=10,random_state=43)\n",
    "    results = []\n",
    "    model_combine_scores_cv = []\n",
    "    model_scores_cv = []\n",
    "    best_params_cv = []\n",
    "    cv_columns = ['label','ensemble']+methods\n",
    "    pred_probs_cv = pd.DataFrame(columns=cv_columns)\n",
    "    predicts_cv = pd.DataFrame(columns=cv_columns)\n",
    "    if not hyperopt:\n",
    "        search_methods = get_train_models(models=methods)\n",
    "    class_num = len(y.unique())\n",
    "    for train_index,test_index in skfolds.split(x,y):\n",
    "        train_fold = x.ix[train_index,:]\n",
    "        train_label = y[train_index]\n",
    "        sample_weight_train = sample_weight[train_index]\n",
    "        test_fold = x.ix[test_index,:]\n",
    "        test_label = y[test_index]\n",
    "        sample_weight_test = sample_weight[test_index]\n",
    "        if up_sampling:\n",
    "            train_fold,train_label,sample_weight_train = upsampling(train_fold,train_label,sample_weight_train,fold=9)\n",
    "            test_fold,test_label,sample_weight_test = downsampling(test_fold,test_label,sample_weight_test)\n",
    "        if not hyperopt:            \n",
    "            for param_l in params.values():\n",
    "                param = param_l[0]\n",
    "                if 'sample_weight' in param:\n",
    "                    param['sample_weight'] = [sample_weight_train,]\n",
    "            ensemble = es.Ensemble(methods=search_methods,params=params)\n",
    "            ensemble.fit(train_fold,train_label)            \n",
    "        else:\n",
    "            ensemble = eh.Ensemble(methods,params)\n",
    "            ensemble.fit(train_fold,train_label,sample_weight=sample_weight_train,max_iter=100)\n",
    "        score = ensemble.score(test_fold,test_label)\n",
    "        results.extend([score])\n",
    "        model_prob = ensemble.model_probs()\n",
    "        model_preds = ensemble.model_predicts() \n",
    "        ensemble_prob = ensemble.predict_proba(test_fold)\n",
    "        ensemble_pred = ensemble.predict(test_fold)\n",
    "        \n",
    "        temp_df = pd.DataFrame(columns=pred_probs_cv.columns)\n",
    "        for method,prob in model_prob.items():\n",
    "            temp_df[method] = prob[:,1]\n",
    "            temp_df[method] = temp_df[method].astype('f')\n",
    "        temp_df['ensemble'] = ensemble_prob[:,1]\n",
    "        temp_df['ensemble'] = temp_df['ensemble'].astype('f')\n",
    "        temp_df['label'] = test_label\n",
    "        temp_df['label'] = temp_df['label'].astype('i8')\n",
    "        pred_probs_cv = pred_probs_cv.append(temp_df,ignore_index=True)\n",
    "        \n",
    "        \n",
    "        temp_df = pd.DataFrame(columns=predicts_cv.columns)\n",
    "        for method,pred in model_preds.items():\n",
    "            temp_df[method] = pred\n",
    "            temp_df[method] = temp_df[method].astype('i8')\n",
    "        temp_df['ensemble'] = ensemble_pred\n",
    "        temp_df['ensemble'] = temp_df['ensemble'].astype('i8')\n",
    "        temp_df['label'] = test_label\n",
    "        temp_df['label'] = temp_df['label'].astype('i8')\n",
    "        predicts_cv = predicts_cv.append(temp_df,ignore_index=True)\n",
    "        \n",
    "        comb_results = methods_combination_results(methods,model_prob,test_label)\n",
    "        model_combine_scores_cv.extend([comb_results])\n",
    "        model_score = ensemble.get_model_scores()\n",
    "        model_scores_cv.extend([model_score.copy()])\n",
    "        best_params = ensemble.best_params()\n",
    "        best_params_cv.extend([best_params.copy()])\n",
    "        all_estimators = list(ensemble.best_estimators_.values())\n",
    "        all_estimators.extend([ensemble])\n",
    "        plot_curves(all_estimators,test_fold,test_label,types='roc_curve')\n",
    "        plot_curves(all_estimators,test_fold,test_label,types='precision_recall_curve')\n",
    "        del ensemble\n",
    "    if class_num == 2:\n",
    "        result_df = pd.DataFrame(results,columns=['logloss','f1','recall','precision','auc_score'])\n",
    "    else:\n",
    "        result_df = pd.DataFrame(results,columns=['logloss','f1','recall','precision'])\n",
    "    return result_df,model_combine_scores_cv,model_scores_cv,best_params_cv,pred_probs_cv,predicts_cv\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "def soft_voting(model_probs):\n",
    "    pred_probs = reduce(lambda x,y: np.add(x,y), model_probs.values())/len(model_probs)\n",
    "    print(pred_probs)\n",
    "    predicts = np.argmax(pred_probs,axis=1)\n",
    "    return pred_probs,predicts    \n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "def get_search_params(methods=['LogisticRegression','RandomForestClassifier','SVC','MLPClassifier','xgbooster','tensor_DNN']):\n",
    "    params={}\n",
    "    feature_num = train_x.shape[1]\n",
    "    #class_weight = {0:1,1:30}\n",
    "    class_weight = None\n",
    "    l_param=[{'C':np.linspace(0.01, 20,20),'class_weight':[class_weight],'sample_weight':[sample_weights_train]}]\n",
    "    rf_param = [{'n_estimators':np.linspace(10,500,5,dtype='i8'),'max_depth':np.linspace(5,30,6,dtype='i8'),'min_samples_split': np.linspace(3,30,5,dtype='i8'),'min_samples_leaf': np.linspace(1,10,10,dtype='i8'),'class_weight':[class_weight],'sample_weight':[sample_weights_train]}]\n",
    "    svc_param = [{'C':np.linspace(0.01,0.2,5),'gamma':np.linspace(0.001,0.5,5),'class_weight':[class_weight],'sample_weight':[sample_weights_train]}]\n",
    "    mlp_param = [{'alpha':np.linspace(0.001,5,10),'max_iter':[3000],'hidden_layer_sizes':[(100,80,50,25,10),(200,120,80,40),(300,200,100),(400,200)]}]\n",
    "    xgb_param = [{'learning_rate':[0.1],'max_depth': np.linspace(3,21,6,dtype='i8'),'n_estimators':np.linspace(500,2000,5,dtype='i8'),'reg_lambda': np.linspace(1,50,10),'gamma':np.linspace(0.1,20,10),'class_weight':[class_weight],'sample_weight':[sample_weights_train],'search':['random',],'n_iter':[20] }]\n",
    "    dnn_param = [{'batch_normalization': [True],\n",
    "                 'l2_reg': np.linspace(0.01,5,5),                            \n",
    "                 'drop_out':np.linspace(0.1,0.8,4),\n",
    "                 'n_classes': [len(train_label.unique())],\n",
    "                 'hidden_layers': [[int(feature_num*5),int(feature_num*3),int(feature_num*1)],[int(feature_num*4),int(feature_num*3),int(feature_num*2),int(feature_num*1)],[int(feature_num*3),int(feature_num*2.5),int(feature_num*2),int(feature_num*1.5),int(feature_num*1)],[int(feature_num*6),int(feature_num*3)]],\n",
    "                 #'weight_factor':np.linspace(1,2,3),\n",
    "                 'steps':np.linspace(200,2000,10,dtype='i8'),\n",
    "                 'batch_size':[30],\n",
    "                 'scoring':['precision'],\n",
    "                 'sample_weight':[sample_weights_train],\n",
    "                 'search':['random',],\n",
    "                 'n_iter':[50]\n",
    "                 }]\n",
    "    if 'LogisticRegression' in methods:\n",
    "        params['LogisticRegression'] = l_param\n",
    "    if 'RandomForestClassifier' in methods:\n",
    "        params['RandomForestClassifier'] = rf_param\n",
    "    if 'SVC' in methods:\n",
    "        params['SVC'] = svc_param\n",
    "    if 'MLPClassifier' in methods:\n",
    "        params['MLPClassifier'] = mlp_param\n",
    "    if 'xgbooster' in methods:\n",
    "        params['xgbooster'] = xgb_param\n",
    "    if 'tensor_DNN' in methods:\n",
    "        params['tensor_DNN'] = dnn_param\n",
    "    return params\n",
    "#-----------------------------------------------------------------------------------\n",
    "def get_train_models(models=['LogisticRegression','RandomForestClassifier','SVC','MLPClassifier','xgbooster','tensor_DNN']):\n",
    "    methods = []\n",
    "    l = LogisticRegression()\n",
    "    rf = RandomForestClassifier()\n",
    "    svc = SVC()\n",
    "    xg = xgbooster.xgbooster()\n",
    "    mlp = MLPClassifier()\n",
    "    dnn = dne.tensor_DNN()\n",
    "    lsvc = LinearSVC()\n",
    "    if 'LogisticRegression' in models:\n",
    "        methods.extend([l])\n",
    "    if 'RandomForestClassifier' in models:\n",
    "        methods.extend([rf])\n",
    "    if 'SVC' in models:\n",
    "        methods.extend([svc])\n",
    "    if 'MLPClassifier' in models:\n",
    "        methods.extend([mlp])\n",
    "    if 'xgbooster' in models:\n",
    "        methods.extend([xg])\n",
    "    if 'tensor_DNN' in models:\n",
    "        methods.extend([dnn])\n",
    "    if 'LinearSVC' in models:\n",
    "        methods.extend([lsvc])\n",
    "    return methods\n",
    " #------------------------------------------------------------------------------\n",
    "\n",
    "def get_hyperopt_params(methods=['LogisticRegression','RandomForestClassifier','LinearSVC','SVC','xgbooster','tensor_DNN','MLPClassifier'],wtf_lo=1,wtf_hi=1):\n",
    "    weight_factor = hp.uniform('weight_factor',wtf_lo,wtf_hi)\n",
    "    params={}\n",
    "    l_param = {'C': hp.uniform('C',0.05,20),'weight_factor':weight_factor}\n",
    "    rf_param = {'n_estimators':100+hp.randint('n_estimators',900),'max_depth':5+hp.randint('max_depth',20), 'min_samples_split': 5+hp.randint('min_samples_split',15),'min_samples_leaf': 2+hp.randint('min_samples_leaf',4),'weight_factor':weight_factor}\n",
    "    svc_param = {'C': hp.uniform('C',0.005,1),'gamma': hp.uniform('gamma',0.001,1),'probability':hp.choice('probability',[True]),'weight_factor':weight_factor}\n",
    "    xgb_param = {'learning_rate':hp.choice('learning_rate',[0.1]),'max_depth': 5+hp.randint('max_depth',15),'n_estimators':500+hp.randint('n_estimators',2000),'reg_lambda': hp.uniform('reg_lambda',20,100),'gamma': hp.uniform('gamma',0.01,10),'weight_factor':weight_factor}\n",
    "    dnn_param = {'batch_normalization': hp.choice('batch_normalization',[True]),\n",
    "                 'l2_reg': hp.uniform('l2_reg',0.001,5),                            \n",
    "                 'drop_out':hp.uniform('drop_out',0.1,0.8),\n",
    "                 'weight_factor':weight_factor,\n",
    "                 'steps':200+hp.randint('steps',1000),\n",
    "                 'batch_size':hp.choice('batch_size',[30]),\n",
    "                 'scoring':hp.choice('scoring',['precision']),\n",
    "                 }\n",
    "    mlp_param = {'alpha':hp.uniform('alpha',0.001,5),'max_iter':2000+hp.randint('max_iter',1000)}\n",
    "    lsvc_param = {'C': hp.uniform('C',0.1,10),'weight_factor':weight_factor} \n",
    "    if 'LogisticRegression' in methods:\n",
    "        params['LogisticRegression'] = l_param\n",
    "    if 'RandomForestClassifier' in methods:\n",
    "        params['RandomForestClassifier'] = rf_param\n",
    "    if 'SVC' in methods:\n",
    "        params['SVC'] = svc_param\n",
    "    if 'xgbooster' in methods:\n",
    "        params['xgbooster'] = xgb_param\n",
    "    if 'tensor_DNN' in methods:\n",
    "        params['tensor_DNN'] = dnn_param\n",
    "    if 'MLPClassifier' in methods:\n",
    "        params['MLPClassifier'] = mlp_param\n",
    "    if 'LinearSVC' in methods:\n",
    "        params['LinearSVC'] = lsvc_param\n",
    "    return params    \n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def methods_combination_results(methods,model_probs,test_label):\n",
    "    n = len(methods)\n",
    "    results = {}\n",
    "    for i in range(1,n+1):\n",
    "        iterator = itertools.combinations(methods,i)\n",
    "        for combination in iterator:\n",
    "            key = reduce(lambda x,y: x+'-'+y,combination)\n",
    "            print(key)\n",
    "            test_model_probs = {method:prob for method,prob in model_probs.items() if method in combination}\n",
    "            pred_probs,pred = soft_voting(test_model_probs)\n",
    "            test_score = scores(test_label,pred,pred_probs[:,1])\n",
    "            results[key] = test_score.copy()\n",
    "    return results\n",
    "#---------------------------------------------------------------------\n",
    "def upsampling(train_x,train_label,sample_weights_train,fold=9):\n",
    "    trainx = train_x.copy()\n",
    "    trainx['label'] = train_label\n",
    "    trainx['weight'] = sample_weights_train\n",
    "    up_samples = commons.upSampling(trainx[trainx['label']==1],10)\n",
    "    trainx = trainx.append(up_samples,ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    train_label = trainx['label']\n",
    "    sample_weights_train = trainx['weight']\n",
    "    trainx = trainx.drop(['label','weight'],axis=1)\n",
    "    return trainx,train_label,sample_weights_train\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def downsampling(x,y,sample_weights):\n",
    "    x = x.copy()\n",
    "    x['label'] = y\n",
    "    x['weight'] = sample_weights\n",
    "    pos = x[x['label']==1]\n",
    "    negs = x[x['label']==0].sample(pos.shape[0])\n",
    "    alls = pos.append(negs,ignore_index=True)\n",
    "    label = alls['label']\n",
    "    weights = alls['weight']\n",
    "    return alls.drop(['label','weight'],axis=1),label,weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import 'selected_features'\n",
    "dataset = 'RICHS'\n",
    "up_sampling = True;\n",
    "if up_sampling:\n",
    "    wtf_lo = 0.05 if dataset==\"RICHS\" else 0.2\n",
    "    wtf_hi = 0.1 if dataset==\"RICHS\" else 0.3\n",
    "else:\n",
    "    wtf_lo = 1.0/3 if dataset==\"RICHS\" else 1 \n",
    "    wtf_hi = 0.5 if dataset==\"RICHS\" else 1.5\n",
    "log_dir = home+'logs/'\n",
    "logger = Logger.Logger(log_dir,False).get_logger()\n",
    "with pd.HDFStore(home+'data/'+dataset+'/selected_features','r') as h5s:\n",
    "    train_x =h5s['train_x'] \n",
    "    train_label = h5s['train_label'] \n",
    "    test_x = h5s['test_x'] \n",
    "    test_label = h5s['test_label']\n",
    "    sample_weights_train = h5s['sample_weights_train'] \n",
    "    sample_weights_test = h5s['sample_weights_test']\n",
    "logger.info('Features used in training are from traditional feature selection')\n",
    "#upsampling train_x and train_label and decreasing sample_weight to 2:1\n",
    "train_x,train_label,sample_weights_train = upsampling(train_x,train_label,sample_weights_train,fold=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model hyperparameter tuning and evaluation\n",
    "methods = ['LogisticRegression','xgbooster']\n",
    "params = get_hyperopt_params(methods,wtf_lo=wtf_lo,wtf_hi=wtf_hi)\n",
    "ensemble_hyopt = eh.Ensemble(methods,params)\n",
    "ensemble_hyopt.fit(train_x,train_label,sample_weight=sample_weights_train,max_iter=100)\n",
    "pred = ensemble_hyopt.predict(test_x)\n",
    "print(\"predicted number of positives: {}\".format((pred==1).sum()))\n",
    "pred_probs = ensemble_hyopt.predict_proba(test_x)\n",
    "score = scores(test_label,pred,pred_probs[:,1])\n",
    "logger.info('Hyperopt ensemble prediction scores: '+str(score))\n",
    "logger.info('The number of positive sample in test dataset: %d, in prediction: %d',(test_label==1).sum(),(pred==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get combine_result\n",
    "methods = ['LogisticRegression','xgbooster']\n",
    "hyperopt_model_probs = ensemble_hyopt.model_probs()\n",
    "comb_results = methods_combination_results(methods,hyperopt_model_probs,test_label)\n",
    "#error_analysis\n",
    "error_analysis(ensemble_hyopt,test_x,test_label,types=['roc_curve','precision_recall_curve','confusion_matrix'])\n",
    "#plot ROC\\PRC curve\n",
    "all_estimators = list(ensemble_hyopt.best_estimators_.values())\n",
    "all_estimators.extend([ensemble_hyopt])\n",
    "plot_curves(all_estimators,test_x,test_label,types='roc_curve')\n",
    "plot_curves(all_estimators,test_x,test_label,types='precision_recall_curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold\n",
    "#### 10-fold test using the ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine train set and test set\n",
    "total_x = pd.concat([train_x,test_x],ignore_index=True)\n",
    "total_label = pd.concat([train_label,test_label],ignore_index=True)\n",
    "total_sample_weights = pd.concat([sample_weights_train,sample_weights_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get tenfold_crossval_scores,model_combine_scores_cv,model_scores_cv,best_params_cv,pred_probs_cv,predicts_cv \n",
    "#use 'cross_val_ensemble' function\n",
    "methods_cv = ['LogisticRegression','xgbooster']\n",
    "if_hyperopt = True\n",
    "if if_hyperopt:\n",
    "    params_cv = get_hyperopt_params(methods_cv,wtf_lo=wtf_lo,wtf_hi=wtf_hi)\n",
    "else:\n",
    "    params_cv = get_search_params(methods=methods_cv)\n",
    "tenfold_crossval_scores,model_combine_scores_cv,model_scores_cv,best_params_cv,pred_probs_cv,predicts_cv = cross_val_ensemble(total_x,total_label,total_sample_weights,methods_cv,params_cv,fold=10,hyperopt=if_hyperopt,up_sampling=up_sampling)\n",
    "logger.info('10-fold CV of ensemble method results:\\n '+tenfold_crossval_scores.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###change types of the results?\n",
    "predicts_dtype = ['i8']*predicts_cv.shape[1]\n",
    "probs_dtype = ['i8','f']+['f']*(pred_probs_cv.shape[1]-2)\n",
    "for i,col in enumerate(predicts_cv.columns):\n",
    "    predicts_cv[col] = predicts_cv[col].astype(predicts_dtype[i])\n",
    "    pred_probs_cv[col] = pred_probs_cv[col].astype(probs_dtype[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get avg_scores\n",
    "avg_score_columns = ['ensemble']+methods_cv\n",
    "avg_scores = {}\n",
    "for method in avg_score_columns:\n",
    "    avg_scores[method] = scores(predicts_cv['label'],predicts_cv[method],pred_probs_cv[method])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ROC and PRC curves\n",
    "plot_curves_cv(pred_probs_cv.drop(['label'],axis=1),pred_probs_cv['label'],methods=['ensemble','LogisticRegression','xgbooster'])\n",
    "plot_curves_cv(pred_probs_cv.drop(['label'],axis=1),pred_probs_cv['label'],methods=['ensemble','LogisticRegression','xgbooster'],types='precision_recall_curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tenfold_crossval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_scores_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_combine_scores_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_xgb_cores = [x['LogisticRegression-xgbooster'] for x in model_combine_scores_cv]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions defination\n",
    "#--------------------------------------------------------------------------------\n",
    "def plot_curves_cv(probs,label,methods,types='roc_curve'):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(types)\n",
    "    plt.axis([0,1,0,1])\n",
    "    lw = 2\n",
    "    colors = ['r','b','g','k','c','m','y']\n",
    "    for method,color in zip(methods,colors[:len(methods)]):\n",
    "        if types == 'precision_recall_curve':       \n",
    "            precision,recall,threshold = precision_recall_curve(label,probs[method])\n",
    "            plt.plot(recall,precision,color,linewidth=2,label=method)\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "        if types == 'roc_curve':\n",
    "            fpr,tpr, threshold = roc_curve(label,probs[method])\n",
    "            plt.plot(fpr,tpr,color,linewidth=2,label=method)\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('Recall')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "def get_estimators(methods,params,train_x,train_label):\n",
    "    ensemble = eh.Ensemble(methods,params)\n",
    "    ensemble.fit(train_x,train_label,sample_weight=sample_weight_train,max_iter=100)\n",
    "    return ensemble\n",
    "#-----------------------------------------------------------------------------\n",
    "def plot_curves(estimators,test_x,label,types='roc_curve'):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(types)\n",
    "    plt.axis([0,1,0,1])\n",
    "    lw = 2\n",
    "    colors = ['r','b','g','k','c','m','y']\n",
    "    for color,estimator in zip(colors[:len(estimators)],estimators):\n",
    "        name = type(estimator).__name__\n",
    "        probs = np.array(estimator.predict_proba(test_x))[:,1]\n",
    "        if types == 'precision_recall_curve':       \n",
    "            precision,recall,threshold = precision_recall_curve(label,probs)\n",
    "            plt.plot(recall,precision,color,linewidth=2,label=name)\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "        if types == 'roc_curve':\n",
    "            fpr,tpr, threshold = roc_curve(label,probs)\n",
    "            plt.plot(fpr,tpr,color,linewidth=2,label=name)\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('Recall')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.legend(loc='best')\n",
    "#-----------------------------------------------------------------------------\n",
    "def learn_curve(model,train_x,train_label,cv=3,scoring='neg_log_loss'):\n",
    "    model_c = clone(model)\n",
    "    N,train_score,test_score = learning_curve(model_c, \n",
    "                                            train_x,train_label,cv=cv,train_sizes=np.linspace(0.5,1,5),\n",
    "                                            scoring=scoring)\n",
    "    \n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.title('{}'.format(type(model).__name__))\n",
    "    plt.plot(N,np.mean(train_score,1),color='blue', label='training score')\n",
    "    plt.plot(N,np.mean(test_score,1),color='red',label='validation score')\n",
    "    plt.xlabel('training sample')\n",
    "    plt.ylabel(scoring)\n",
    "    plt.legend(loc=0)\n",
    "    plt.show()\n",
    "#-----------------------------------------------------------------------------    \n",
    "def error_analysis(estimator,test_x,label,types=['confusion_matrix']):\n",
    "    print('Error analysis of ',type(estimator).__name__)\n",
    "    predict = estimator.predict(test_x)\n",
    "    probs = np.array(estimator.predict_proba(test_x))[:,1]\n",
    "    class_num = len(label.unique())\n",
    "    if 'confusion_matrix' in types:\n",
    "        conf_mat = confusion_matrix(label,predict)\n",
    "        row_sums = conf_mat.sum(axis=1,keepdims=True)\n",
    "        norm_conf_mat = conf_mat/row_sums\n",
    "        np.fill_diagonal(norm_conf_mat,0)\n",
    "        plt.matshow(norm_conf_mat,cmap=plt.cm.gray)        \n",
    "    if 'precision_recall_curve' in types and class_num<=2:\n",
    "        precision,recall,threshold = precision_recall_curve(label,probs)\n",
    "        plot_curve(recall,precision,type(estimator).__name__,'precision_recall_curve')\n",
    "    if 'roc_curve' in types and class_num<=2:\n",
    "        fpr,tpr, threshold = roc_curve(label,probs)\n",
    "        plot_curve(fpr,tpr,type(estimator).__name__,'roc_curve')    \n",
    "    plt.show()\n",
    "#---------------------------------------------------------------------------    \n",
    "def plot_curve(score1,score2,label,types):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(types)\n",
    "    plt.plot(score1,score2,linewidth=2,label=label)\n",
    "    plt.plot([0,1],[1,1],'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    lw = 2\n",
    "    if types == 'precision_recall_curve':        \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "    if types == 'roc_curve':\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "#--------------------------------------------------------------------------\n",
    "def scores(y,predicts,pred_probs,average='macro'):\n",
    "    class_num = len(y.unique())\n",
    "    score_map = {}\n",
    "\n",
    "    if class_num <=2:\n",
    "        recall = recall_score(y,predicts)      \n",
    "        precision = precision_score(y,predicts)      \n",
    "        accuracy = accuracy_score(y,predicts)\n",
    "        f1 = f1_score(y,predicts)\n",
    "        auc = roc_auc_score(y,pred_probs)\n",
    "        score_map['auc'] = auc\n",
    "    else:\n",
    "        recall = recall_score(y,predicts,average=average)      \n",
    "        precision = precision_score(y,predicts,average=average)      \n",
    "        accuracy = accuracy_score(y,predicts)\n",
    "        f1 = f1_score(y,predicts,average=average)\n",
    "        \n",
    "    score_map['recall'] = recall\n",
    "    score_map['precision'] = precision\n",
    "    score_map['accuracy'] = accuracy\n",
    "    score_map['f1'] = f1\n",
    "    return score_map\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "def cross_val_ensemble(x,y,sample_weight,methods,params,fold=10, hyperopt=True,up_sampling=False):\n",
    "    skfolds = StratifiedKFold(n_splits=10,random_state=43)\n",
    "    results = []\n",
    "    model_combine_scores_cv = []\n",
    "    model_scores_cv = []\n",
    "    best_params_cv = []\n",
    "    cv_columns = ['label','ensemble']+methods\n",
    "    pred_probs_cv = pd.DataFrame(columns=cv_columns)\n",
    "    predicts_cv = pd.DataFrame(columns=cv_columns)\n",
    "    if not hyperopt:\n",
    "        search_methods = get_train_models(models=methods)\n",
    "    class_num = len(y.unique())\n",
    "    for train_index,test_index in skfolds.split(x,y):\n",
    "        train_fold = x.ix[train_index,:]\n",
    "        train_label = y[train_index]\n",
    "        sample_weight_train = sample_weight[train_index]\n",
    "        test_fold = x.ix[test_index,:]\n",
    "        test_label = y[test_index]\n",
    "        sample_weight_test = sample_weight[test_index]\n",
    "        if up_sampling:\n",
    "            train_fold,train_label,sample_weight_train = upsampling(train_fold,train_label,sample_weight_train,fold=9)\n",
    "            test_fold,test_label,sample_weight_test = downsampling(test_fold,test_label,sample_weight_test)\n",
    "        if not hyperopt:            \n",
    "            for param_l in params.values():\n",
    "                param = param_l[0]\n",
    "                if 'sample_weight' in param:\n",
    "                    param['sample_weight'] = [sample_weight_train,]\n",
    "            ensemble = es.Ensemble(methods=search_methods,params=params)\n",
    "            ensemble.fit(train_fold,train_label)            \n",
    "        else:\n",
    "            ensemble = eh.Ensemble(methods,params)\n",
    "            ensemble.fit(train_fold,train_label,sample_weight=sample_weight_train,max_iter=100)\n",
    "        score = ensemble.score(test_fold,test_label)\n",
    "        results.extend([score])\n",
    "        model_prob = ensemble.model_probs()\n",
    "        model_preds = ensemble.model_predicts() \n",
    "        ensemble_prob = ensemble.predict_proba(test_fold)\n",
    "        ensemble_pred = ensemble.predict(test_fold)\n",
    "        \n",
    "        temp_df = pd.DataFrame(columns=pred_probs_cv.columns)\n",
    "        for method,prob in model_prob.items():\n",
    "            temp_df[method] = prob[:,1]\n",
    "            temp_df[method] = temp_df[method].astype('f')\n",
    "        temp_df['ensemble'] = ensemble_prob[:,1]\n",
    "        temp_df['ensemble'] = temp_df['ensemble'].astype('f')\n",
    "        temp_df['label'] = test_label\n",
    "        temp_df['label'] = temp_df['label'].astype('i8')\n",
    "        pred_probs_cv = pred_probs_cv.append(temp_df,ignore_index=True)\n",
    "        \n",
    "        \n",
    "        temp_df = pd.DataFrame(columns=predicts_cv.columns)\n",
    "        for method,pred in model_preds.items():\n",
    "            temp_df[method] = pred\n",
    "            temp_df[method] = temp_df[method].astype('i8')\n",
    "        temp_df['ensemble'] = ensemble_pred\n",
    "        temp_df['ensemble'] = temp_df['ensemble'].astype('i8')\n",
    "        temp_df['label'] = test_label\n",
    "        temp_df['label'] = temp_df['label'].astype('i8')\n",
    "        predicts_cv = predicts_cv.append(temp_df,ignore_index=True)\n",
    "        \n",
    "        comb_results = methods_combination_results(methods,model_prob,test_label)\n",
    "        model_combine_scores_cv.extend([comb_results])\n",
    "        model_score = ensemble.get_model_scores()\n",
    "        model_scores_cv.extend([model_score.copy()])\n",
    "        best_params = ensemble.best_params()\n",
    "        best_params_cv.extend([best_params.copy()])\n",
    "        all_estimators = list(ensemble.best_estimators_.values())\n",
    "        all_estimators.extend([ensemble])\n",
    "        plot_curves(all_estimators,test_fold,test_label,types='roc_curve')\n",
    "        plot_curves(all_estimators,test_fold,test_label,types='precision_recall_curve')\n",
    "        del ensemble\n",
    "    if class_num == 2:\n",
    "        result_df = pd.DataFrame(results,columns=['logloss','f1','recall','precision','auc_score'])\n",
    "    else:\n",
    "        result_df = pd.DataFrame(results,columns=['logloss','f1','recall','precision'])\n",
    "    return result_df,model_combine_scores_cv,model_scores_cv,best_params_cv,pred_probs_cv,predicts_cv\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "def soft_voting(model_probs):\n",
    "    pred_probs = reduce(lambda x,y: np.add(x,y), model_probs.values())/len(model_probs)\n",
    "    print(pred_probs)\n",
    "    predicts = np.argmax(pred_probs,axis=1)\n",
    "    return pred_probs,predicts    \n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def get_train_models(models=['LogisticRegression','RandomForestClassifier','SVC','MLPClassifier','xgbooster','tensor_DNN']):\n",
    "    methods = []\n",
    "    l = LogisticRegression()\n",
    "    rf = RandomForestClassifier()\n",
    "    svc = SVC()\n",
    "    xg = xgbooster.xgbooster()\n",
    "    mlp = MLPClassifier()\n",
    "    dnn = dne.tensor_DNN()\n",
    "    lsvc = LinearSVC()\n",
    "    if 'LogisticRegression' in models:\n",
    "        methods.extend([l])\n",
    "    if 'RandomForestClassifier' in models:\n",
    "        methods.extend([rf])\n",
    "    if 'SVC' in models:\n",
    "        methods.extend([svc])\n",
    "    if 'MLPClassifier' in models:\n",
    "        methods.extend([mlp])\n",
    "    if 'xgbooster' in models:\n",
    "        methods.extend([xg])\n",
    "    if 'tensor_DNN' in models:\n",
    "        methods.extend([dnn])\n",
    "    if 'LinearSVC' in models:\n",
    "        methods.extend([lsvc])\n",
    "    return methods\n",
    " #------------------------------------------------------------------------------\n",
    "\n",
    "def get_hyperopt_params(methods=['LogisticRegression','RandomForestClassifier','LinearSVC','SVC','xgbooster','tensor_DNN','MLPClassifier'],wtf_lo=1,wtf_hi=1):\n",
    "    weight_factor = hp.uniform('weight_factor',wtf_lo,wtf_hi)\n",
    "    weight_factor = hp.uniform('weight_factor',wtf_lo,wtf_hi)\n",
    "    params={}\n",
    "    l_param = {'C': hp.uniform('C',0.05,20),'weight_factor':weight_factor}\n",
    "    rf_param = {'n_estimators':100+hp.randint('n_estimators',900),'max_depth':5+hp.randint('max_depth',20), 'min_samples_split': 5+hp.randint('min_samples_split',15),'min_samples_leaf': 2+hp.randint('min_samples_leaf',4),'weight_factor':weight_factor}\n",
    "    svc_param = {'C': hp.uniform('C',0.005,1),'gamma': hp.uniform('gamma',0.001,1),'probability':hp.choice('probability',[True]),'weight_factor':weight_factor}\n",
    "    xgb_param = {'learning_rate':hp.choice('learning_rate',[0.1]),'max_depth': 5+hp.randint('max_depth',15),'n_estimators':500+hp.randint('n_estimators',2000),'reg_lambda': hp.uniform('reg_lambda',20,100),'gamma': hp.uniform('gamma',0.01,10),'weight_factor':weight_factor}\n",
    "    dnn_param = {'batch_normalization': hp.choice('batch_normalization',[True]),\n",
    "                 'l2_reg': hp.uniform('l2_reg',0.001,5),                            \n",
    "                 'drop_out':hp.uniform('drop_out',0.1,0.8),\n",
    "                 'weight_factor':weight_factor,\n",
    "                 'steps':200+hp.randint('steps',1000),\n",
    "                 'batch_size':hp.choice('batch_size',[30]),\n",
    "                 'scoring':hp.choice('scoring',['precision']),\n",
    "                 }\n",
    "    mlp_param = {'alpha':hp.uniform('alpha',0.001,5),'max_iter':2000+hp.randint('max_iter',1000)}\n",
    "    lsvc_param = {'C': hp.uniform('C',0.1,10),'weight_factor':weight_factor} \n",
    "    if 'LogisticRegression' in methods:\n",
    "        params['LogisticRegression'] = l_param\n",
    "    if 'RandomForestClassifier' in methods:\n",
    "        params['RandomForestClassifier'] = rf_param\n",
    "    if 'SVC' in methods:\n",
    "        params['SVC'] = svc_param\n",
    "    if 'xgbooster' in methods:\n",
    "        params['xgbooster'] = xgb_param\n",
    "    if 'tensor_DNN' in methods:\n",
    "        params['tensor_DNN'] = dnn_param\n",
    "    if 'MLPClassifier' in methods:\n",
    "        params['MLPClassifier'] = mlp_param\n",
    "    if 'LinearSVC' in methods:\n",
    "        params['LinearSVC'] = lsvc_param\n",
    "    return params    \n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def methods_combination_results(methods,model_probs,test_label):\n",
    "    n = len(methods)\n",
    "    results = {}\n",
    "    for i in range(1,n+1):\n",
    "        iterator = itertools.combinations(methods,i)\n",
    "        for combination in iterator:\n",
    "            key = reduce(lambda x,y: x+'-'+y,combination)\n",
    "            print(key)\n",
    "            test_model_probs = {method:prob for method,prob in model_probs.items() if method in combination}\n",
    "            pred_probs,pred = soft_voting(test_model_probs)\n",
    "            test_score = scores(test_label,pred,pred_probs[:,1])\n",
    "            results[key] = test_score.copy()\n",
    "    return results\n",
    "#---------------------------------------------------------------------\n",
    "def upsampling(train_x,train_label,sample_weights_train,fold=9):\n",
    "    trainx = train_x.copy()\n",
    "    trainx['label'] = train_label\n",
    "    trainx['weight'] = sample_weights_train\n",
    "    up_samples = commons.upSampling(trainx[trainx['label']==1],10)\n",
    "    trainx = trainx.append(up_samples,ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    train_label = trainx['label']\n",
    "    sample_weights_train = trainx['weight']\n",
    "    trainx = trainx.drop(['label','weight'],axis=1)\n",
    "    return trainx,train_label,sample_weights_train\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def downsampling(x,y,sample_weights):\n",
    "    x = x.copy()\n",
    "    x['label'] = y\n",
    "    x['weight'] = sample_weights\n",
    "    pos = x[x['label']==1]\n",
    "    negs = x[x['label']==0].sample(pos.shape[0])\n",
    "    alls = pos.append(negs,ignore_index=True)\n",
    "    label = alls['label']\n",
    "    weights = alls['weight']\n",
    "    return alls.drop(['label','weight'],axis=1),label,weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import data for all features\n",
    "dataset = 'RICHS'\n",
    "start = str(prediction_commons.tss_start)\n",
    "end = str(prediction_commons.tss_end)\n",
    "all_features = home+'data/WGBS/all_features_'+str(start)+'_'+str(end)\n",
    "model_path = home+'data/'+dataset+'/prediction_model.pkl'\n",
    "pred_probs = home+'data/'+dataset+'/pred_probs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features selecetd by traditional methods\n",
    "up_sampling = True;\n",
    "if up_sampling:\n",
    "    wtf_lo = 0.05 if dataset==\"RICHS\" else 0.2\n",
    "    wtf_hi = 0.1 if dataset==\"RICHS\" else 0.3\n",
    "else:\n",
    "    wtf_lo = 1.0/3 if dataset==\"RICHS\" else 1 \n",
    "    wtf_hi = 0.5 if dataset==\"RICHS\" else 1.5\n",
    "    \n",
    "log_dir = home+'logs/'\n",
    "logger = Logger.Logger(log_dir,False).get_logger()\n",
    "with pd.HDFStore(home+'data/'+dataset+'/selected_features','r') as h5s:\n",
    "    train_x =h5s['train_x'] \n",
    "    train_label = h5s['train_label'] \n",
    "    test_x = h5s['test_x'] \n",
    "    test_label = h5s['test_label']\n",
    "    sample_weights_train = h5s['sample_weights_train'] \n",
    "    sample_weights_test = h5s['sample_weights_test']\n",
    "total_x = pd.concat([train_x,test_x],ignore_index=True)\n",
    "total_label = pd.concat([train_label,test_label],ignore_index=True)\n",
    "total_sample_weights = pd.concat([sample_weights_train,sample_weights_test],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model\n",
    "methods = ['LogisticRegression','xgbooster']\n",
    "params = get_hyperopt_params(methods,wtf_lo=wtf_lo,wtf_hi=wtf_hi)\n",
    "ensemble_hyopt = eh.Ensemble(methods,params)\n",
    "ensemble_hyopt.fit(total_x,total_label,sample_weight=total_sample_weights,max_iter=100)\n",
    "joblib.dump(ensemble_hyopt,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get WGBS data for selected features\n",
    "selected_features = total_x.columns\n",
    "with pd.HDFStore(all_features,'r') as h5s:\n",
    "    wgbs_all_data = h5s['all_features']\n",
    "wgbs_data = wgbs_all_data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict\n",
    "ensemble_hyopt = joblib.load(model_path)\n",
    "for method,best_estimator in ensemble_hyopt.best_estimators_.items():\n",
    "    best_estimator.fit(total_x,total_label,total_sample_weights)\n",
    "pred = ensemble_hyopt.predict(wgbs_data)\n",
    "pred_prob = ensemble_hyopt.predict_proba(wgbs_data)\n",
    "pred_prob = pd.DataFrame(pred_prob,columns=['negative','positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get target sites where 'positive' > 0.5\n",
    "target_sites = pred_prob.sort_values(['positive'],ascending=False).query('positive > 0.5')\n",
    "target_sites_coordinate = wgbs_all_data.ix[target_sites.index,['chr','coordinate']]\n",
    "target_sites = target_sites.join(target_sites_coordinate)\n",
    "#save target sites\n",
    "with pd.HDFStore(pred_probs,'w') as h5s:\n",
    "    h5s[start+'_'+end] = target_sites"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
