{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from common import commons\n",
    "home = commons.home\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,cross_validate\n",
    "from common import Convert_To_Normal_Dist as ctnd\n",
    "from features_selection import TTest as tt\n",
    "from matplotlib import pyplot as plt\n",
    "from features_selection import CorrFeatureSelector as cfs\n",
    "from simulation import AutoencoderSimulator as AS\n",
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from features_selection import Feature_Selection as FS\n",
    "from common import DataScaler as ds\n",
    "from log import Logger\n",
    "from feature_engineering import Sparse_Autoencoder_class as sac\n",
    "from hyperopt import fmin,tpe,hp, STATUS_OK,Trials\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from features_selection import TTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------- \n",
    "def normalize_feature(all_features,dataset='AD_CpG'):\n",
    "    count_thresh = 10\n",
    "    unique_value_counts = [len(all_features[col].unique()) for col in all_features.columns[2:]] #ignore chr and coordinate\n",
    "    normalize_all_features = all_features.copy()\n",
    "    mappings = pd.DataFrame()\n",
    "    for col,num in zip(all_features.columns[2:],unique_value_counts):\n",
    "        if num >= count_thresh:\n",
    "            print(col)\n",
    "            atn = ctnd.AnyToNormal(col)\n",
    "            mapping = atn.transform(normalize_all_features)\n",
    "            mappings[mapping.columns[0]] = mapping.ix[:,0]\n",
    "            mappings[mapping.columns[1]] = mapping.ix[:,1]\n",
    "        else: \n",
    "            continue\n",
    "    h5s = pd.HDFStore(home+'data/'+dataset+'/normalize_mapping','w')\n",
    "    h5s['mappings'] = mappings\n",
    "    h5s['normal_features'] = normalize_all_features\n",
    "    h5s.close()\n",
    "    return home+'data/'+dataset+'/normalize_mapping',normalize_all_features,mappings\n",
    "   \n",
    "#-------------------------------------------------------------------------------\n",
    "def data_selection(data,classes=[0,1,-1],combine=False,*args):              #which classes of data to keep\n",
    "    if not isinstance(data,pd.DataFrame):\n",
    "        cols = args\n",
    "        data = pd.DataFrame(data,columns=cols)    \n",
    "    ret_data = data.query('label in @classes')\n",
    "    for i,label in enumerate(classes):\n",
    "        ret_data['label'][ret_data['label']==label] = i\n",
    "    \n",
    "    if len(classes)>2 and combine==True:\n",
    "        ret_data['label'] = ret_data['label'].where(ret_data['label']==0,1)\n",
    "    return ret_data\n",
    "        \n",
    "#------------------------------------------------------------------------------                  \n",
    " \n",
    "def TSNEPlot(data,class_labels,param_map):##param_map like 1:('r','o','80')\n",
    "    data1 = data.reset_index()\n",
    "    tsne = TSNE(n_components=3,random_state=91)\n",
    "    feature_reduced = tsne.fit_transform(data1.drop(['label','index'],axis=1))\n",
    "    fig = plt.figure(figsize=(18,15))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    for i in class_labels:\n",
    "        c,marker,size = param_map[i]\n",
    "        print(c,marker,size)\n",
    "        index = data1.query('label == @i').index.values\n",
    "        ax.scatter(feature_reduced[index,0],feature_reduced[index,1],feature_reduced[index,2],c=c,marker=marker,s=size)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def simulate(data,label,**argkw):\n",
    "    data = data[data['label']==label]\n",
    "    encoder = AS.dataset_simulator(**argkw)\n",
    "    encoder.fit(np.array(data.drop('label',axis=1)))\n",
    "    sim_data = pd.DataFrame(encoder.transform(),columns=data.columns.drop('label'))\n",
    "    sim_data['label'] = label\n",
    "    return sim_data\n",
    "          \n",
    "#-----------------------------------------------------------------------------\n",
    "def subset_control(data,num):      ## randomly select a subset of certain ratio positive vs control samples \n",
    "    pos_index = data.query('label!=0').index.values\n",
    "    control_sub_index = np.random.permutation(data.query('label==0').index.values)[:len(pos_index)*num]\n",
    "    select_index = np.concatenate((pos_index,control_sub_index))\n",
    "    return data.loc[select_index,:]\n",
    "#------------------------------------------------------------------------------\n",
    "def sparse_autoencoder_score(ae_params):\n",
    "    global encoders\n",
    "    sparse_ae = sac.sparse_autoencoder(**ae_params)\n",
    "    score = commons.cross_validate_score(sparse_ae,reduced_train_x,train_label,ae_sample_weights_train)\n",
    "    encoders.extend([sparse_ae])\n",
    "    if math.isnan(score) or math.isfinite(score):\n",
    "        score = np.Infinity\n",
    "    return {'loss':-score,'status':STATUS_OK}\n",
    "#-----------------------------------------------------------------\n",
    "def selected_feature_analysis(features,X,y):\n",
    "    t_stats = []\n",
    "    pos = X[y==1]\n",
    "    neg = X[y==0]\n",
    "    for feature in features:\n",
    "        test = TTest.FeatureTTest(feature)\n",
    "        t_stats.extend([test.fit(pos,neg)])\n",
    "    return pd.DataFrame(t_stats)\n",
    "#--------------------------------------------------------------------\n",
    "def method_params(methods=['random_forest','xgboost','logistic_regression','linear_SVC']):\n",
    "    params={}\n",
    "    feature_num = train_x.shape[1]\n",
    "    #class_weight = {0:1,1:30}\n",
    "    class_weight = None\n",
    "    l_param={'C':9,'penalty':'l1'}\n",
    "    rf_param = {'n_estimators':1000,'max_depth':10,'min_samples_split':14 ,'min_samples_leaf':1, 'n_jobs':-1 }\n",
    "    svc_param = {'C':2,'dual':False,'penalty':'l1'}\n",
    "    xgb_param = {'learning_rate':0.1,'max_depth':10,'n_estimators':1500,'reg_lambda':40,'gamma':1,'n_jobs':-1}\n",
    "    mutual_information_param = {'k':100}\n",
    "    fisher_param = {'k':100}\n",
    "    if 'logistic_regression' in methods:\n",
    "        params['logistic_regression'] = l_param\n",
    "    if 'random_forest' in methods:\n",
    "        params['random_forest'] = rf_param\n",
    "    if 'linear_SVC' in methods:\n",
    "        params['linear_SVC'] = svc_param\n",
    "    if 'xgboost' in methods:\n",
    "        params['xgboost'] = xgb_param\n",
    "    if 'mutual_information' in methods:\n",
    "        params['mutual_information'] = mutual_information_param\n",
    "    if 'fisher_score' in methods:\n",
    "        params['fisher_score'] = fisher_param   \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "``/home/ec2-user/git/EnsembleCpG/data/AD_CpG/all_features`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-552e09b9d7b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhome\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'logs/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'data/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/all_features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta_sign'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fletcher32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfletcher32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'can not be written'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m# Finally, create the File instance, and return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tables/file.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tables/utils.py\u001b[0m in \u001b[0;36mcheck_file_access\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# The file should be readable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF_OK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"``%s`` does not exist\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"``%s`` is not a regular file\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ``/home/ec2-user/git/EnsembleCpG/data/AD_CpG/all_features`` does not exist"
     ]
    }
   ],
   "source": [
    "dataset = 'AD_CpG'\n",
    "log_dir = home+'logs/'\n",
    "logger = Logger.Logger(log_dir,False).get_logger()\n",
    "with pd.HDFStore(home+'data/'+dataset+'/all_features','r') as h5s:\n",
    "    all_data = h5s['all_features']\n",
    "all_data['beta_sign'] = all_data['label']\n",
    "#all_data['coordinate'] = all_data['coordinate'].astype('i8')\n",
    "all_data.drop(['coordinate','chr'],axis=1,inplace=True)\n",
    "all_data['dist_to_nearest_tss'] = all_data['dist_to_nearest_tss'].astype('i8')\n",
    "all_data = data_selection(all_data,classes=[0,1,-1],combine=True)\n",
    "#all_data = data_selection(all_data,classes=[0,1,-1],combine=True)\n",
    "all_features = all_data\n",
    "#all_features = subset_control(all_data,30)\n",
    "#all_features = all_data.query('beta_sign>0') ##only for hypermethylated sites in RICHS dataset, for AD dataset, hyper/hypo status can't be determined from beta\n",
    "#logger.info('only keep heypermethylated sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split train test data and scaling on train data\n",
    "scaler='MinMax'\n",
    "all_features.drop(['id','winid','beta','beta_sign'],axis=1,inplace=True)\n",
    "train_x,train_label,test_x,test_label = commons.train_test_split(all_features,scaler=scaler)\n",
    "train_x.reset_index(drop=True,inplace=True)\n",
    "train_label.reset_index(drop=True,inplace=True)\n",
    "test_x.reset_index(drop=True,inplace=True)\n",
    "test_label.reset_index(drop=True,inplace=True)\n",
    "logger.info('Data scaler type is: %s',scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights_train = commons.sample_weights(train_x,train_label,factor=1)\n",
    "sample_weights_test = commons.sample_weights(test_x,test_label,factor=1)\n",
    "weight_min_max_ratio = sample_weights_train.max()/sample_weights_train.min()\n",
    "logger.info('weight max ratio: %f',weight_min_max_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.605507916274266"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_min_max_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x.drop(['pvalue'],axis=1,inplace=True)\n",
    "test_x.drop(['pvalue'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "fs_sample_weights = np.power(sample_weights_train,0.5) if dataset == 'RICHS' else sample_weights_train\n",
    "methods = ['random_forest','xgboost','logistic_regression','linear_SVC']\n",
    "all_intersect = False\n",
    "fs_params = method_params(methods)\n",
    "fs = FS.FeatureSelection(class_num=2,methods=methods,all_intersect=all_intersect,**fs_params)\n",
    "logger.info('Feature selection methods are: '+str(methods))\n",
    "logger.info('All intersected features: '+str(all_intersect))\n",
    "fs.fit(sample_weight=fs_sample_weights)\n",
    "selected_features = fs.transform(train_x,train_label)\n",
    "logger.info('selected features number is: %d\\n',selected_features.shape[0])\n",
    "logger.info(selected_features)\n",
    "reduced_train_x = train_x[selected_features['feature']]\n",
    "reduced_test_x = test_x[selected_features['feature']]\n",
    "total_x = pd.concat([reduced_train_x,reduced_test_x],ignore_index=True)\n",
    "total_label = pd.concat([train_label,test_label],ignore_index=True)\n",
    "total_weights = pd.concat([sample_weights_train,sample_weights_test],ignore_index=True)\n",
    "\n",
    "feature_diff_stats = selected_feature_analysis(selected_features['feature'],total_x,total_label)\n",
    "feature_diff_stats = pd.merge(feature_diff_stats,selected_features,on='feature')\n",
    "selected_features_100 = feature_diff_stats if len(feature_diff_stats) <=50 else feature_diff_stats.sort_values(['n','pvalue'],ascending=[False,True])[:50]\n",
    "reduced_train_x = train_x[selected_features_100['feature']]\n",
    "reduced_test_x = test_x[selected_features_100['feature']]\n",
    "total_x = pd.concat([reduced_train_x,reduced_test_x],ignore_index=True)\n",
    "total_label = pd.concat([train_label,test_label],ignore_index=True)\n",
    "total_weights = pd.concat([sample_weights_train,sample_weights_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_diff_stats.sort_values(['n','pvalue'],ascending=[False,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_diff_stats.to_csv(home+'data/'+dataset+'feature_stas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TSNEPlot(plot_data,class_labels=[0,1,2],param_map={0:('g','^',20),1:('b','*',20),2:('r','o',20)})\n",
    "plot_data = reduced_train_x.copy()\n",
    "plot_data['label'] = train_label\n",
    "TSNEPlot(plot_data,class_labels=[0,1],param_map={0:('k','^',20),1:('r','*',20)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(home+'data/'+dataset+'/selected_features','w') as h5s:\n",
    "    h5s['train_x'] = reduced_train_x\n",
    "    h5s['train_label'] = train_label\n",
    "    h5s['test_x'] = reduced_test_x\n",
    "    h5s['test_label'] = test_label\n",
    "    h5s['sample_weights_train'] = sample_weights_train\n",
    "    h5s['sample_weights_test'] = sample_weights_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
