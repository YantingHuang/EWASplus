{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ec2-user/CpGPython/code/')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "def kl_divergence(p,q):\n",
    "    return p*tf.log(p/q)+(1-p)*tf.log((1-p)/(1-q))\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "def get_learning_rate(initial_rate=0.1,decay_steps=10000,decay_rate=1/10):\n",
    "    global_step = tf.Variable(0,trainable=False,name='global_step')\n",
    "    learning_rate = tf.train.exponential_decay(initial_rate,global_step,decay_steps,decay_rate)\n",
    "    return learning_rate\n",
    "\n",
    "#-----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home='/home/ec2-user/CpGPython/'\n",
    "reg_log_dir = home+'logs/'\n",
    "logger = Logger.Logger(reg_log_dir,False).get_logger()\n",
    "with pd.HDFStore(home+'data/selected_features','r') as h5s:\n",
    "    train_x =h5s['train_x'] \n",
    "    train_label = h5s['train_label'] \n",
    "    test_x = h5s['test_x'] \n",
    "    test_label = h5s['test_label']   \n",
    "    sample_weights_train = h5s['sample_weights_train'] \n",
    "    sample_weights_test = h5s['sample_weights_test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_log_dir = home+'tensor_logs/' \n",
    "log_dir = \"{}run-{}\".format(root_log_dir,datetime.utcnow().strftime(\"%Y%m%d%H%M%S\"))\n",
    "log_writer = tf.summary.FileWriter(log_dir,tf.get_default_graph())\n",
    "   \n",
    "n_input = train_x.shape[1]\n",
    "n_hidden1 = int(1.5*n_input)\n",
    "n_output = n_input\n",
    "l2_reg = 0.01\n",
    "sparsity_target = 0.01\n",
    "sparsity_weight = 0.2\n",
    "top_k = 30\n",
    "logger.info('Sparse autencoder: input layer node number: %d\\n hidden layer number is: 1\\n \\\n",
    "            hidden layer node number is: %d\\n L2 regularization: %f\\n \\\n",
    "            sparsity target: %f\\n sparsity penalty: %f\\n selected number of features: %d',n_input,n_hidden1,l2_reg,\n",
    "            sparsity_target,sparsity_weight,top_k)\n",
    "train_x = MinMaxScaler(copy=True,feature_range=(0,1)).fit_transform(train_x)\n",
    "test_x = MinMaxScaler().fit_transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation = tf.nn.sigmoid\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "dense_layer = partial(tf.layers.dense,kernel_regularizer=regularizer,kernel_initializer=he_init)\n",
    "inputs = tf.placeholder(tf.float32,shape=(None,n_input),name='inputs')\n",
    "sample_weights = tf.placeholder(tf.float32,shape=(None),name='weights')\n",
    "training = tf.placeholder_with_default(False,shape=(),name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = dense_layer(inputs,n_hidden1,name='hidden1')\n",
    "    bn1 = tf.layers.batch_normalization(hidden1,training=training,momentum=0.9)\n",
    "    bn1_act = activation(bn1)\n",
    "    hidden1_mean = tf.reduce_mean(bn1_act,axis=0)\n",
    "    logits_before_bn = dense_layer(bn1_act,n_output,name='outputs',activation=None)\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn,training=training,momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "with tf.name_scope('loss'):\n",
    "    reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)    \n",
    "    reconstruction_loss = tf.reduce_sum(tf.nn.weighted_cross_entropy_with_logits(targets=inputs,logits=logits,pos_weight=sample_weights))\n",
    "    sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target,hidden1_mean))\n",
    "    loss = tf.add_n([reconstruction_loss,sparsity_loss]+reg_loss,name='loss')\n",
    "    reconstruction_loss_summary = tf.summary.scalar('logloss',reconstruction_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------    \n",
    "with tf.name_scope('train'):\n",
    "    learning_rate = get_learning_rate()\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9,use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_epochs = 50\n",
    "n_batch = 20\n",
    "batch_size = train_x.shape[0]//n_batch\n",
    "logger.info('Training epochs: %d, training batches: %d, batch_size: %d',n_epochs,n_batch,batch_size)\n",
    "saver = tf.train.Saver() \n",
    "sample_weights_train.reset_index(drop=True,inplace=True)\n",
    "sample_weights_test.reset_index(drop=True,inplace=True)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.75142835e-04   1.89812417e-04   1.00000000e+00 ...,   2.90107837e-05\n",
      "    6.02610235e-04   7.66981742e-04]\n",
      " [  1.96966842e-01   7.96413042e-06   2.20743345e-06 ...,   4.40971735e-06\n",
      "    7.46686816e-01   5.24982903e-03]\n",
      " [  9.76211846e-01   2.02843206e-04   3.07010814e-05 ...,   1.79100280e-05\n",
      "    5.82559884e-01   2.98234692e-04]\n",
      " ..., \n",
      " [  2.29470793e-13   5.07725948e-14   9.54134334e-15 ...,   2.41967825e-07\n",
      "    1.89910235e-13   2.28022863e-08]\n",
      " [  1.97556004e-01   6.95512805e-04   1.14361323e-01 ...,   2.59221797e-05\n",
      "    1.02499407e-03   2.42193323e-03]\n",
      " [  2.98912615e-01   3.66853172e-04   1.95805825e-07 ...,   5.61964089e-06\n",
      "    3.10334587e-03   1.06405416e-04]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        random_ix = np.random.permutation(train_x.shape[0]) \n",
    "        for iteration in range(n_batch):\n",
    "            start_ix = iteration*batch_size \n",
    "            end_ix = np.minimum((iteration+1)*batch_size,train_x.shape[0])\n",
    "            ixs = random_ix[start_ix:end_ix]\n",
    "            x_batch = train_x[ixs,:]\n",
    "            weights_batch = sample_weights_train[ixs].values.reshape([-1,1])\n",
    "            if iteration == 0:\n",
    "                summary_str = reconstruction_loss_summary.eval(feed_dict={inputs:x_batch,sample_weights:weights_batch,training:True})\n",
    "                step = n_epochs*n_batch\n",
    "                log_writer.add_summary(summary_str,step)\n",
    "                logger.info(\"Loss at epoch %d: %f\",epoch,loss.eval(feed_dict={inputs:x_batch,sample_weights:weights_batch,training:True}))\n",
    "           # print(hidden1_mean.eval(feed_dict={inputs:x_batch,sample_weights:weights_batch,training:True}))\n",
    "            sess.run([training_op,update_ops],feed_dict={inputs:x_batch,sample_weights:weights_batch,training:True})\n",
    "    logger.info(\"Loss at end: %f\",loss.eval(feed_dict={inputs:x_batch,sample_weights:weights_batch,training:True}))\n",
    "    total_x = np.concatenate([train_x,test_x])\n",
    "    total_label = np.concatenate([np.array(train_label),np.array(test_label)])\n",
    "    total_weights = pd.concat([sample_weights_train,sample_weights_test],ignore_index=True).values.reshape((-1,1))\n",
    "    new_features = bn1_act.eval(feed_dict={inputs:total_x,sample_weights:total_weights})\n",
    "    print(new_features)\n",
    "    saver.save(sess,'/home/ec2-user/tensor_model/sparse_autoencoder.ckpt')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_features_mean = np.mean(new_features,axis=0)\n",
    "max_active_ix = np.argpartition(new_features_mean,top_k)[:-top_k-1:-1]\n",
    "new_features1 = new_features[:,max_active_ix]\n",
    "new_features1 = StandardScaler().fit_transform(new_features1)\n",
    "split = StratifiedShuffleSplit(n_splits=1,test_size=0.1,random_state=17)\n",
    "for train_index, test_index in split.split(new_features1,total_label):\n",
    "    train_set = pd.DataFrame(new_features1[train_index])\n",
    "    test_set = pd.DataFrame(new_features1[test_index])\n",
    "    train_label = pd.Series(total_label[train_index])\n",
    "    test_label = pd.Series(total_label[test_index])\n",
    "    sample_weights_train = pd.DataFrame(total_weights[train_index])[0]\n",
    "    sample_weights_test = pd.DataFrame(total_weights[test_index])[0]\n",
    "with pd.HDFStore(home+'data/new_features','w') as h5s:\n",
    "    h5s['train_x'] = train_set\n",
    "    h5s['train_label'] = train_label\n",
    "    h5s['test_x'] = test_set\n",
    "    h5s['test_label'] = test_label\n",
    "    h5s['sample_weights_train'] = sample_weights_train\n",
    "    h5s['sample_weights_test'] = sample_weights_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44611529, -0.28089699,  0.58303916, ..., -1.11837137,\n",
       "        -0.31271407, -0.2425992 ],\n",
       "       [ 1.42341995, -0.78983718,  1.61772645, ...,  0.69567531,\n",
       "        -0.31271434, -0.2425992 ],\n",
       "       [-0.44995463, -0.60869014,  0.32352176, ..., -0.78662258,\n",
       "        -0.31271434, -0.23492101],\n",
       "       ..., \n",
       "       [-1.50923538, -0.76276058, -0.68531454, ...,  1.50223434,\n",
       "         3.52411079, -0.2425992 ],\n",
       "       [-0.61526322, -0.76695049, -0.68435878, ..., -1.08873916,\n",
       "        -0.3127141 , -0.24259892],\n",
       "       [ 0.1860432 , -0.64420503,  1.90069449, ..., -0.99953932,\n",
       "        -0.31270862, -0.2425992 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
